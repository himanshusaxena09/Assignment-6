{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FrtywVQiw3P"
      },
      "outputs": [],
      "source": [
        "# What is the main purpose of RCNN in object detection\n",
        "\n",
        "# The main purpose of R-CNN (Region-based Convolutional Neural Network) in object detection is to accurately detect and classify objects in an image by\n",
        "# combining region proposals with deep feature extraction using a CNN.\n",
        "\n",
        "# Selecting regions in an image that are likely to contain objects.\n",
        "# Extracting deep features from those regions using a CNN.\n",
        "# Classifying each region into object categories like â€œcat,â€ â€œcar,â€ etc. or background."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between Fast RCNN and Faster RCNN\n",
        "\n",
        "# Fast R-CNN\n",
        "\n",
        "# Region Proposal --  Method\tUses an external algorithm like Selective Search (slow and hand-engineered).\n",
        "# Speed\t-- Slower (â‰ˆ2 seconds per image).\n",
        "# End-to-End Training\t--  Partially end-to-end â€” CNN and classifier are trained together, but region proposals come from outside.\n",
        "# Accuracy\t--  Good, but limited by hand-crafted region proposals.\n",
        "# Architecture Complexity\t-- Simpler (depends on external process).\n",
        "\n",
        "# Faster R-CNN\n",
        "\n",
        "# Region Proposal --  Uses a built-in Region Proposal Network (RPN) (learned and fast).\n",
        "# Speed\t--  Much faster (â‰ˆ0.2 seconds per image).\n",
        "# End-to-End Training\t--  Fully end-to-end â€” region proposals and detection are trained jointly.\n",
        "# Accuracy\t--  Better accuracy due to RPNâ€™s high-quality proposals.\n",
        "# Architecture Complexity\t--  Slightly more complex (adds RPN), but more efficient."
      ],
      "metadata": {
        "id": "DVDy6NajkgJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How does YOLO handle object detection in real-time\n",
        "\n",
        "# YOLO handles object detection in real-time by framing detection as a single regression problem â€” directly predicting bounding boxes and class\n",
        "# probabilities from the full image in one forward pass of the network.\n",
        "\n",
        "# Single-pass architecture\t-- The image is processed in one go.\n",
        "# End-to-end CNN\t-- Fully convolutional â€” optimized for GPUs.\n",
        "# Shared computation\t-- Predictions are made from shared feature maps.\n",
        "# Optimized design\t-- Small and efficient model architecture (especially in later versions like YOLOv4, v5, v8)."
      ],
      "metadata": {
        "id": "CIp2mT9coBzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain the concept of Region Proposal Networks (RPN) in Faster RCNN\n",
        "\n",
        "# A Region Proposal Network (RPN) is a small fully convolutional network inside Faster R-CNN that automatically generates candidate object regions\n",
        "# (region proposals) directly from feature maps â€” replacing slow, hand-crafted methods like Selective Search.\n",
        "\n",
        "# Faster R-CNN introduced the RPN to:\n",
        "# Eliminate external region proposal algorithms.\n",
        "# Make the entire system end-to-end trainable and GPU-accelerated.\n",
        "\n",
        "# Speed\t--  Removes slow Selective Search â†’ proposals generated in milliseconds.\n",
        "# Accuracy\t-- Learns proposals directly from data, improving localization."
      ],
      "metadata": {
        "id": "XMe-cn1rr974"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How does YOLOv9 improve upon its predecessor\n",
        "\n",
        "# You get better accuracy (higher mAP) for a given model size or compute budget compared to under-optimised predecessors.\n",
        "# You get better resource efficiency: same accuracy with fewer parameters or fewer FLOPs, or you can scale up to get higher accuracy if you have more\n",
        "# compute.\n",
        "# You get more flexibility: model variants tuned for devices with different constraints (edge/IoT vs server) along with improvements in training\n",
        "# stability.\n",
        "# In deployment contexts (real-time object detection, constrained hardware), the improvements make YOLOv9 more viable or better performing than older\n",
        "# versions."
      ],
      "metadata": {
        "id": "p3kfTo4FtSuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What role does non-max suppression play in YOLO object detection\n",
        "\n",
        "# In YOLO, the model often predicts multiple bounding boxes for the same object because:\n",
        "# Multiple grid cells may detect the same object.\n",
        "# Overlapping boxes may have similar confidence scores.\n",
        "\n",
        "# So, Non-Max Suppression (NMS) is used to:\n",
        "# Remove duplicate bounding boxes and\n",
        "# Keep only the most confident one for each detected object.\n",
        "\n",
        "# NMS Works\n",
        "# YOLO generates multiple predictions\n",
        "# Filter boxes by confidence threshold\n",
        "# Sort remaining boxes\n",
        "# Suppress overlapping boxes\n",
        "# Repeat until all boxes are processed"
      ],
      "metadata": {
        "id": "03saoB317snQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Describe the data preparation process for training YOLOv9\n",
        "\n",
        "# The data preparation process for YOLOv9 involves:\n",
        "\n",
        "# Collect Images\n",
        "# Annotate Data\n",
        "# Organize Folders\tFollow YOLO folder structure\n",
        "# Create data.yaml\tDefine paths and classes\n",
        "# Augment Data\tIncrease diversity and robustness\n",
        "# Split Dataset\tTrain/Val/Test for performance check\n",
        "# Validate Dataset\tEnsure labels are correct\n",
        "# Train Model"
      ],
      "metadata": {
        "id": "QUNE-AXl8wXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What is the significance of anchor boxes in object detection models like YOLOv9\n",
        "\n",
        "# Anchor boxes are predefined bounding box shapes (width & height ratios) that act as reference templates for object detection models.\n",
        "# Each anchor box represents a typical object aspect ratio and scale (for example, tall, wide, or square objects).\n",
        "\n",
        "# When YOLO predicts bounding boxes:\n",
        "# It doesnâ€™t start from scratch â€” it adjusts (offsets) these anchor boxes to better fit the actual object.\n",
        "\n",
        "# Real-world images contain objects of various sizes and shapes\n",
        "# If the model tried to predict every box directly from pixels, it would struggle with this diversity.\n",
        "# Anchor boxes give the model a starting point for different object types, helping it learn faster and more accurately."
      ],
      "metadata": {
        "id": "zO-ogtd_-V8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What is the key difference between YOLO and R-CNN architectures\n",
        "\n",
        "# YOLO (You Only Look Once)\n",
        "\n",
        "# Single-stage detector\n",
        "# YOLO divides the image into a grid and directly predicts bounding boxes and class probabilities for each grid cell in one forward pass of the network.\n",
        "# Perform object localization + classification simultaneously â€” all at once.\n",
        "# Very fast (real-time performance).\n",
        "\n",
        "\n",
        "# R-CNN (Region-based CNN)\n",
        "\n",
        "# Two-stage detector\n",
        "\n",
        "# Generate region proposals (potential object areas).\n",
        "# Pass each proposal through a CNN to classify and refine the bounding box.\n",
        "# Achieve high detection accuracy, even if slower.\n",
        "# Slower, but often more precise."
      ],
      "metadata": {
        "id": "Woxm-DxKFd3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is Faster RCNN considered faster than Fast RCNN\n",
        "\n",
        "# Fast R-CNN\n",
        "\n",
        "# Uses Selective Search to generate ~2000 candidate boxes per image.\n",
        "# This algorithm is hand-crafted and CPU-bound, taking a significant portion of processing time.\n",
        "# CNN feature extraction is efficient, but the proposal step dominates inference time.\n",
        "\n",
        "# Faster R-CNN\n",
        "\n",
        "# Introduces a Region Proposal Network (RPN):\n",
        "# Small CNN sliding over feature maps.\n",
        "# Predicts region proposals directly from CNN feature maps.\n",
        "# Runs on GPU, fully integrated with the detection network.\n",
        "# Eliminates the slow external proposal step."
      ],
      "metadata": {
        "id": "f6DFJjYbH_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the role of selective search in RCNN\n",
        "\n",
        "# In R-CNN, Selective Search generates candidate object regions (region proposals) so that the CNN only needs to process likely object locations\n",
        "# instead of the entire image. Itâ€™s essentially the first step in detecting â€œwhere to lookâ€.\n",
        "\n",
        "# Selective Search is used to generate region proposals â€” candidate bounding boxes that are likely to contain objects in the image.\n",
        "# In R-CNN, instead of checking every possible window (which would be computationally huge), Selective Search reduces the search space to a manageable\n",
        "# number of likely regions."
      ],
      "metadata": {
        "id": "Arl9h5TzJV_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How does YOLOv9 handle multiple classes in object detection\n",
        "\n",
        "# YOLOv9 treats object detection as a single regression problem, where each grid cell predicts bounding boxes and class probabilities simultaneously.\n",
        "\n",
        "# So for multiple classes:\n",
        "# Each predicted box includes class scores for all classes in the dataset.\n",
        "# The model selects the class with the highest probability for each detected object.\n",
        "\n",
        "# Multi-Class Prediction\n",
        "# YOLOv9 supports any number of classes.\n",
        "# Training is multi-class aware, using cross-entropy or BCE loss per class.\n",
        "# The network can detect multiple objects of different classes in a single image simultaneously."
      ],
      "metadata": {
        "id": "OBGZDNn0JfKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What are the key differences between YOLOv3 and YOLOv9\n",
        "\n",
        "# Aspect\t                YOLOv3\t                        YOLOv9\n",
        "\n",
        "# Backbone\t              Darknet-53\t                    GELAN\n",
        "# Detection\t              Anchor-based, 3 scales\t        Anchor-based / anchor-free, multi-scale fusion\n",
        "# Training\t              Standard BCE + IoU\t            Advanced losses + PGI\n",
        "# Speed\t                  Real-time (~30 FPS)\t            Faster and scalable variants\n",
        "# Accuracy\t              Good\t                          Higher, especially for small/dense objects\n",
        "# Multi-class handling\t  Standard\t                      Improved with dynamic matching"
      ],
      "metadata": {
        "id": "pDU1zvE3gTIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How is the loss function calculated in Faster RCNN\n",
        "\n",
        "# RPN Loss = Objectness (binary classification) + Bounding box regression (smooth L1)\n",
        "# Fast R-CNN Loss = Multi-class classification + Bounding box regression\n",
        "# Total Loss = ð¿ð‘…ð‘ƒð‘+ð¿ð‘‘ð‘’ð‘¡LRPN\tâ€‹+Ldet\tâ€‹â†’ optimized end-to-end\n",
        "\n",
        "# In simple terms: Faster R-CNNâ€™s loss ensures the model learns both where objects are and what they are, for both the region proposal\n",
        "# stage and final detection stage."
      ],
      "metadata": {
        "id": "wBIxXm6DnEEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain how YOLOv9 improves speed compared to earlier versions\n",
        "\n",
        "# More efficient backbone / feature-aggregation network\n",
        "# Improved training / gradient dynamics so smaller/heavier models are more efficient\n",
        "# Model scaling & variant choices\n",
        "# Reduced computation / parameters while maintaining accuracy\n",
        "# Better multi-scale / feature fusion / detection head improvements"
      ],
      "metadata": {
        "id": "UAH9nyZao0hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are some challenges faced in training YOLOv9\n",
        "\n",
        "# Complex architecture (PGI, GELAN)     Harder to tune, unstable gradients\n",
        "# Data quality issues                   Poor accuracy, bia\n",
        "# High compute needs                    Long training time\n",
        "# Speedâ€“accuracy trade-off              Suboptimal model for task\n",
        "# Hyperparameter sensitivity            Training instability\n",
        "# Overfitting                           Poor generalization\n",
        "# Anchor tuning                         Localization errors\n",
        "# Domain shift                          Low transfer performance"
      ],
      "metadata": {
        "id": "jeYCK4JpoiD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the YOLOv9 architecture handle large and small object detection\n",
        "\n",
        "# Multi-Scale Feature Extraction with GELAN Backbone\n",
        "# Path Aggregation Network (PAN) Neck for Feature Fusion\n",
        "# Multi-Scale Detection Heads\n",
        "# Adaptive Anchor Boxes\n",
        "# Programmable Gradient Information (PGI)\n",
        "# Improved Data Augmentation\n",
        "# Balanced Loss Function"
      ],
      "metadata": {
        "id": "_6U8cxt8CtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the significance of fine-tuning in YOLO\n",
        "\n",
        "# Fine-tuning in YOLO plays a crucial role in adapting a pre-trained model to a new or specific dataset\n",
        "\n",
        "# Domain adaptation -- Learn features specific to your custom dataset\n",
        "# Faster convergence -- Start from pre-learned weights\n",
        "# Higher accuracy -- Better detection with limited data\n",
        "# Prevent overfitting -- Use pre-trained general features wisely\n",
        "# Transfer learning -- Reuse learned knowledge efficiently"
      ],
      "metadata": {
        "id": "7rrLGKhyIj3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the concept of bounding box regression in Faster RCNN\n",
        "\n",
        "# Bounding Box Regression is a process where the network learns to predict precise coordinates of the objectâ€™s bounding box refining rough region proposals into\n",
        "# accurate object boundaries.\n",
        "\n",
        "# Faster R-CNN doesnâ€™t start with perfect bounding boxes.\n",
        "# Instead, it begins with anchor boxes.\n",
        "# Bounding box regression helps the network adjust these anchor boxes to better fit the true object."
      ],
      "metadata": {
        "id": "u4plMAy-O6m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Describe how transfer learning is used in YOLO\n",
        "\n",
        "# Transfer learning means using a model thatâ€™s already trained on a large dataset and fine-tuning it for a new, usually smaller, task.\n",
        "# In YOLO, this means you donâ€™t train the entire model from scratch â€” instead, you start from a pre-trained YOLO model.\n",
        "\n",
        "# Load pre-trained weights.\n",
        "# Replace the detection head to match your custom dataset.\n",
        "# Fine-tune on your dataset."
      ],
      "metadata": {
        "id": "5S83nacFHtU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What is the role of the backbone network in object detection models like YOLOv9\n",
        "\n",
        "# A backbone is the initial part of the neural network that processes the input image and extracts low-level to high-level visual features such as:\n",
        "# Edges, corners (low-level)\n",
        "# Shapes, textures (mid-level)\n",
        "# Objects, patterns (high-level)\n",
        "\n",
        "# These extracted features are then passed to the neck and head of the YOLO model for further processing and detection.\n",
        "\n",
        "# Its key roles include:\n",
        "\n",
        "# a) Feature Extraction\n",
        "# b) Hierarchical Representation\n",
        "# c) Reusing Pre-trained Weights\n",
        "# d) Balancing Accuracy & Speed"
      ],
      "metadata": {
        "id": "knDTIaQFIy7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How does YOLO handle overlapping objects\n",
        "\n",
        "# When multiple objects are close together or overlapping, YOLOâ€™s model may predict multiple bounding boxes for the same object.\n",
        "\n",
        "# For example:\n",
        "# If two people are standing close together, the detector might output 4â€“5 boxes â€” two for each person and a few duplicate boxes due to overlapping regions."
      ],
      "metadata": {
        "id": "ZP6-32GlKiWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the importance of data augmentation in object detection\n",
        "\n",
        "# Data augmentation means artificially increasing your dataset size by applying transformations to existing images â€” while keeping the labels (bounding boxes, classes)\n",
        "# consistent.\n",
        "# In object detection, this not only modifies the image but also adjusts the bounding box coordinates accordingly.\n",
        "# Example:\n",
        "# If you flip an image horizontally, the bounding boxes must also flip.\n",
        "\n",
        "# Data Augmentation is Important in Object Detection\n",
        "# a) Improves Generalization\n",
        "# b) Prevents Overfitting\n",
        "# c) Improves Robustness to Real-World Variations\n",
        "# d) Balances the Dataset"
      ],
      "metadata": {
        "id": "-5bST7eCLge1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How is performance evaluated in YOLO-based object detection\n",
        "\n",
        "# In object detection, performance is measured by how accurately and how precisely the model:\n",
        "\n",
        "# Identifies the correct class of an object (classification)\n",
        "# Places an accurate bounding box around it (localization)"
      ],
      "metadata": {
        "id": "XI09aBBpM05v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do the computational requirements of Faster RCNN compare to those of YOLO\n",
        "\n",
        "# Faster R-CNN (Two-Stage Approach)\n",
        "\n",
        "# Stage 1 â€” Region Proposal Network (RPN):\n",
        "# Scans the image and proposes ~2000 potential regions (RoIs).\n",
        "# Each region is evaluated for â€œobjectness.â€\n",
        "# Stage 2 â€” ROI Pooling + Classifier:\n",
        "# Each region proposal is processed again by a CNN to classify and refine bounding boxes.\n",
        "\n",
        "# YOLO (Single-Stage Approach)\n",
        "\n",
        "# Divides the image into a grid.\n",
        "# Each grid cell directly predicts:\n",
        "# Bounding box coordinates\n",
        "# Objectness score\n",
        "# Class probabilities\n",
        "\n",
        "# Faster R-CNN is computationally heavier and slower because it performs detection in two stages, while YOLO is lightweight, faster, and real-time capable due to its\n",
        "# single-stage, fully convolutional design."
      ],
      "metadata": {
        "id": "bIiTHq33ObCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What role do convolutional layers play in object detection with RCNN\n",
        "\n",
        "# Convolutional layers in CNNs (Convolutional Neural Networks) automatically learn to detect:\n",
        "# Edges (from early layers)\n",
        "# Shapes and textures (from middle layers)\n",
        "# Objects or parts of objects (from deeper layers)\n",
        "# These extracted features help the R-CNN framework identify what objects are present and where they are in the image."
      ],
      "metadata": {
        "id": "RpAQ4VfqP6Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the loss function in YOLO differ from other object detection models\n",
        "\n",
        "# The loss function tells the model how wrong its predictions are compared to the ground truth.\n",
        "# the loss typically measures three main things:\n",
        "\n",
        "# Localization (Bounding box regression) â†’ Are the predicted boxes correctly placed?\n",
        "# Confidence (Objectness) â†’ Is there really an object there?\n",
        "# Classification â†’ Is the detected object of the correct class?\n",
        "\n",
        "# YOLOâ€™s loss function is unified and end-to-end, combining localization, confidence, and classification losses in a single step â€” unlike other object detection models\n",
        "#  (like Faster R-CNN), which use multiple loss functions for different stages (proposal and classification).\n",
        "\n",
        "# This makes YOLO faster, simpler to train, and ideal for real-time detection."
      ],
      "metadata": {
        "id": "dPfe4z1ISBfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the key advantages of using YOLO for real-time object detection\n",
        "\n",
        "# 1. Real-Time Speed\n",
        "# 2. Single-Stage, End-to-End Architecture\n",
        "# 3. Global Context Awareness\n",
        "# 4. Good Accuracyâ€“Speed Trade-off\n",
        "# 5. Flexible and Scalable\n",
        "# 6. Improved Bounding Box Precision\n",
        "# 7. Data Efficiency and Transfer Learning"
      ],
      "metadata": {
        "id": "jr0OSrlkTCQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does Faster RCNN handle the trade-off between accuracy and speed\n",
        "\n",
        "# Faster R-CNN handles the accuracyâ€“speed trade-off mainly by using a Region Proposal Network (RPN) to speed up region generation and shared convolutional features to\n",
        "# reduce redundant computation â€” while keeping its two-stage design to maintain high accuracy.\n",
        "\n",
        "# Itâ€™s slower than YOLO but more accurate and reliable for tasks needing fine localization (e.g., medical imaging, document layout detection, etc.)."
      ],
      "metadata": {
        "id": "TfVrafGIUERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ\n",
        "\n",
        "# The backbone is the feature extractor in any object detection model.\n",
        "# It takes a raw input image and converts it into a feature map â€” a high-level, compact representation containing edges, textures, shapes, and object patterns.\n",
        "\n",
        "# Think of it as the \"eyes\" of the network that see and understand the image before the detection heads decide what and where the objects.\n",
        "\n",
        "# The backbone in YOLO is optimized for speed and global feature extraction, feeding directly into detection heads, while the backbone in Faster R-CNN is designed for depth\n",
        "# and precision, supporting its two-stage region proposal + classification process.\n",
        "\n",
        "# YOLOâ€™s backbone makes it ideal for real-time tasks whereas Faster R-CNNâ€™s backbone makes it better for high-detail, accuracy-critical tasks."
      ],
      "metadata": {
        "id": "Dyc1rAk-VKeB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOv9)\n",
        "\n",
        "# 1. Install the Required Library\n",
        "# 2. Import and Load the Model\n",
        "# 3. Run Inference on a Custom Image\n",
        "# 4. Display Predictions Programmatically\n",
        "# 5. Access Detection Details\n",
        "\n",
        "# You load your YOLOv8/YOLOv9 model using the YOLO() class from Ultralytics, then run inference with .predict() on any custom image or video. The results include bounding\n",
        "# boxes, labels, confidence scores, and can be displayed or saved easily."
      ],
      "metadata": {
        "id": "Qp5BwrfNVGTI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture\n",
        "\n",
        "# 1. Import Dependencies\n",
        "# 2. Load the Pretrained Faster R-CNN Model\n",
        "# 3. Print the Model Architecture\n",
        "# 4. Check the Model Summary"
      ],
      "metadata": {
        "id": "Fgxh39yWnXQ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you perform inference on an online image using the Faster RCNN model and print the predictions\n",
        "\n",
        "# 1. Install Dependencies\n",
        "# 2. Import Required Modules\n",
        "# 3. Load the Pretrained Faster R-CNN Model\n",
        "# 4. Load an Online Image\n",
        "# 5. Define Image Transformations\n",
        "# 6. Run Inference\n",
        "# 7. Inspect Predictions\n",
        "# 8. Visualize Predictions"
      ],
      "metadata": {
        "id": "BIdQQ9gUoSEq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you load an image and perform inference using YOLOv9, then display the detected objects with bounding boxes and class labels\n",
        "\n",
        "# 1. Install YOLO (Ultralytics)\n",
        "# 2. Import Required Libraries\n",
        "# 3. Load the YOLOv9 Model\n",
        "# 4. Load an Image\n",
        "# 5. Run Inference\n",
        "# 6. Display Predictions (with Bounding Boxes & Labels)\n",
        "# 7. Print Detected Objects"
      ],
      "metadata": {
        "id": "JwL5_fYKnNT_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you display bounding boxes for the detected objects in an image using Faster RCNN\n",
        "\n",
        "# 1. Install Dependencies\n",
        "# 2. Import Libraries\n",
        "# 3. Load the Pretrained Faster R-CNN Model\n",
        "# 4. Load and Preprocess the Image\n",
        "# 5. Run Inference\n",
        "# 6. Extract Predictions\n",
        "# 7. Display Bounding Boxes and Labels"
      ],
      "metadata": {
        "id": "Vo-cctY9q746"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you perform inference on a local image using Faster RCNN\n",
        "\n",
        "# Load pretrained Faster R-CNN --\tfasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "# Load local image --\tImage.open(\"image.jpg\")\n",
        "# Transform --\tConvert to tensor\n",
        "# Inference --\tmodel([img_tensor])\n",
        "# Visualize --\tUse matplotlib to draw boxes & labels"
      ],
      "metadata": {
        "id": "dIAnFnKKcIPe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions\n",
        "\n",
        "# Direct parameter --\tmodel(\"image.jpg\", conf=0.5)\t-- Filters during inference\n",
        "# Post-processing --\tif conf > 0.6: --\tFilters after predictions\n",
        "# Visualization\t-- results[0].plot(conf_thres=0.6) -- Shows only confident boxes\n",
        "# CLI option --\tconf=0.6\t-- Use in command-line mode"
      ],
      "metadata": {
        "id": "QLW4hhAec84_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you plot the training and validation loss curves for model evaluation\n",
        "\n",
        "# Train YOLO model --\tmodel.train()\n",
        "# Check auto-generated loss curves --\tresults.png\n",
        "# Load CSV for detailed plots --\tresults.csv\n",
        "# Visualize with Matplotlib --\tplt.plot()\n",
        "# Analyze for overfitting / underfitting --\tCompare trends"
      ],
      "metadata": {
        "id": "PgUFEXkbd6rU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you perform inference on multiple images from a local folder using Faster RCNN and display the bounding boxes for each\n",
        "\n",
        "# Load model --\tPretrained Faster R-CNN on COCO\n",
        "# Transform image --\tConvert to tensor\n",
        "# Run inference --\tmodel([img_tensor])\n",
        "# Filter detections --\tKeep score > threshold\n",
        "# Draw boxes --\tUse OpenCV + Matplotlib\n",
        "# Loop -- through folder\tPerform inference for all local images"
      ],
      "metadata": {
        "id": "hyAL-hheefAd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster RCNN\n",
        "\n",
        "# Task\tCode Snippet\tDescription\n",
        "# Load pretrained model --\tfasterrcnn_resnet50_fpn(pretrained=True) --\tPretrained COCO weights\n",
        "# Transform input --\ttransforms.ToTensor()\t-- Convert to tensor\n",
        "# Filter detections -- if score > threshold\t-- Keep confident ones\n",
        "# Draw labels + scores --\tcv2.putText()\t-- Show text on image\n",
        "# Visualize\t-- matplotlib.pyplot.imshow()\t-- Display results"
      ],
      "metadata": {
        "id": "Suv-3E61fB7K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you save the inference results (with bounding boxes) as a new image after performing detection using YOLO\n",
        "\n",
        "# Single image inference --\tresults = model(\"image.jpg\") --\tRuns detection\n",
        "# Auto-save output --\tsave=True --\tSaves annotated image\n",
        "# Manual save --\tcv2.imwrite(\"output.jpg\", results[0].plot()) --\tSaves your custom output\n",
        "# Custom directory --\tproject=\"output_results\", name=\"exp1\" --\tControls output folder\n",
        "# Batch inference --\tmodel(\"folder/*.jpg\", save=True) --\tDetect on multiple images"
      ],
      "metadata": {
        "id": "EGywjuD7fbJ2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zutk29yZglgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}